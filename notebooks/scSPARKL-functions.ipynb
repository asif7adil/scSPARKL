{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Cell RNA-seq Data analysis using scSPARKL- An Apache Spark based parallel computational tool.\n",
    "\n",
    "scSPARKL- is an exclusive Apache Spark based computational tool developed for the analysis of large scale single-cell transcriptomic data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Import scSPARKL modules.\n",
    "from data_load import read_spark, read_spark_t #use 'read_spark_t' to read a transposed matrix i.e., [gene x cell] type.\n",
    "from data_filter import qc_matrix_cells, qc_matrix_genes, filter_matrix\n",
    "from data_normalize import norm_quantile, norm_global\n",
    "from dimension_reduction import pca_apply, umap_apply, tsne_apply\n",
    "from clustering_functions import cluster_data_prep, k_means_spark\n",
    "from differential_expression import diff_expr_top_n\n",
    "from top_hvg import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File loading\n",
    "\n",
    "The framework supports `.csv` file formats for now. Also, the input should be in the form of `cell x gene` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"./data/jurkat_raw_data_annot.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this line to transpose the data\n",
    "df = pd.read_csv(path_to_file, index_col=0)\n",
    "df = df.transpose()\n",
    "df.to_csv(path_to_file.split('.csv')[0]+'_t.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read new input files\n",
    "Following line reads the files as spark dataframe. The function `read_spark()` performs following tasks:\n",
    "- Reads a `.csv` file as a `Spark Dataframe`\n",
    "- Performs Cleaning of unknown characters for `spark` and replaces them with an `Underscore`.\n",
    "- Melts the dataframe from `wide` fromat to `tall` format.\n",
    "- Writes and returns the cleaned dataframe to and as a `parquet` format.\n",
    "\n",
    "<br>Completed stages are printed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df, df_melt = read_spark(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read previously saved parquet data files form the disc storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df= spark.read.parquet('./interim/df/')\n",
    "df_melt = spark.read.parquet('./interim/df_melt/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Cell and Gene Quality summary\n",
    "The output is written as a `.csv` in the `analyses` folder.\n",
    "<br>\n",
    "<br>**Note:** The input is a melted spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate Cell Quality Summary.\n",
    "df_qc_cells = qc_matrix_cells(df_melt)\n",
    "df_qc_cells.toPandas().to_csv('./analyses/qc_cells.csv', index=False)\n",
    "\n",
    "# Generate Gene Quality Summary.\n",
    "df_qc_genes = qc_matrix_genes(df_melt)\n",
    "df_qc_genes.toPandas().to_csv('./analyses/qc_genes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the summaries for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the spark datframes to pandas to visualize\n",
    "pd_df_qc_cells = df_qc_cells.toPandas()\n",
    "pd_df_qc_genes = df_qc_genes.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd_df_qc_cells.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd_df_qc_genes.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(pd_df_qc_cells['total_number_of_columns'], bins=50)\n",
    "plt.xlabel('N genes')\n",
    "plt.ylabel('N cells')\n",
    "plt.axvline(2000, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pd_df_qc_cells['sum_of_all_entries'], bins=1000)\n",
    "plt.xlabel('Total counts')\n",
    "plt.ylabel('N cells')\n",
    "#plt.axvline(10000, color='red')\n",
    "plt.xlim(0,1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pd_df_qc_cells['percentage_of_ercc'], bins=50)\n",
    "plt.xlabel('Percent counts ERCC')\n",
    "plt.ylabel('N cells')\n",
    "plt.axvline(10, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pd_df_qc_genes['no_cells_by_counts'], bins=100)\n",
    "plt.xlabel('N cells expressing > 0')\n",
    "plt.ylabel('log(N genes)') # for visual clarity\n",
    "plt.axvline(2, color='red')\n",
    "plt.yscale('log') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pd_df_qc_genes['sum_of_cells'], bins=100)\n",
    "plt.xlabel('Total counts')\n",
    "plt.ylabel('log(N genes)')\n",
    "plt.yscale('log') \n",
    "plt.axvline(10, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pd_df_qc_cells['percentage_of_mt'], bins=50)\n",
    "plt.xlabel('percent mitochondria')\n",
    "plt.ylabel('N cells')\n",
    "#plt.axvline(2000, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pd_df_qc_genes['no_of_dropouts'], bins=50)\n",
    "plt.xlabel('dropouts')\n",
    "plt.ylabel('N genes')\n",
    "#plt.axvline(2000, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out the unwanted Cells and Genes\n",
    "The `filter_matrix()` uses following as the default values for the removal:\n",
    "<br>\n",
    "- ERCC percentage > 10%'\n",
    "- Mitochondrial percentage > 5%\n",
    "- Cells having < 10 genes where count is 1\n",
    "- Genes expressing in < 3 cells\n",
    "- Dropout rate > 95%\n",
    "\n",
    "**Note: filtering thresholds can be changed (as desired) in the `data_filter` package** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_melt_flt = filter_matrix(df_melt,df_qc_cells,None, apply_ercc = True, apply_mito = True) #cell filtering applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_melt_flt = filter_matrix(df_melt_flt,None,df_qc_genes) #gene filtering applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write filtered matrix to drive as a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt_flt.write.parquet('./interim/df_melt_flt/', mode='overwrite')\n",
    "df_melt_flt = spark.read.parquet('./interim/df_melt_flt/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'cells':df_melt_flt.select('cell').drop_duplicates().count(),\n",
    "'genes':df_melt_flt.select('variable').drop_duplicates().count()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "We currently implement two types of normalizations:\n",
    "- Quantile Normalization https://doi.org/10.1038/s41598-020-72664-6\n",
    "- Global/simple Normalization/CPM Normalization\n",
    "<br>\n",
    "The output of the normalizations is in `wide` format as well as in `tall` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_norm, df_norm_melt = norm_global(df_melt_flt) #norm_global is similar to cpm normalizaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write global normalization\n",
    "df_norm.write.parquet('./interim/df_norm_G/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_g = spark.read.parquet('./interim/df_norm_G/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of Highly Variable Genes\n",
    "There are two methods for selecting top HVGs:\n",
    "- Coefficient of Variances squared. Takes 'n' as a parameter, for returning `n` number of HVG genes.\n",
    "- Median Absoluute Deviatioin. Takes 'k' as a parameter of threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_hvg = top_hvg(df_norm_melt, calc_cv2=True, n=18000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**persist the dataframe and check the count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist\n",
    "top_hvg = top_hvg.persist()\n",
    "top_hvg.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the dataframe\n",
    "Perform PCA & Visualize\n",
    "<br> Perform Kmeans based on first 2 PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_pca, pca_components, pca_components_cummulative = pca_apply(top_hvg,k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.write.parquet('./interim/df_pca/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = spark.read.parquet('./interim/df_pca/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PCA Components\n",
    "x_axis = range(len(pca_components))\n",
    "y_lim = max(pca_components_cummulative)*1.2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_axis, pca_components, color=\"C0\")\n",
    "ax.set_ylim([0, y_lim])\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(range(len(pca_components)), pca_components_cummulative, color=\"C1\", marker=\"D\", ms=1)\n",
    "ax2.set_ylim([0, y_lim])\n",
    "\n",
    "ax.tick_params(axis=\"y\", colors=\"C0\")\n",
    "ax2.tick_params(axis=\"y\", colors=\"C1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract components- PCA (only required for PCA as PCA output is Spark Vector)\n",
    "df_plot = df_pca.toPandas()\n",
    "\n",
    "df_plot['x_comp'] = df_plot['components'].apply(lambda x: x[x_comp])\n",
    "df_plot['y_comp'] = df_plot['components'].apply(lambda x: x[y_comp])\n",
    "\n",
    "# plot PCA: update the 'hue' column as required, or skip it if not required\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.scatterplot(x=df_plot['x_comp'], y=df_plot['y_comp'], hue=df_plot['sp_nme'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kmeans**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust = df_pca # in case no extra steps were performed on PCA output df<br>\n",
    "df_clust = cluster_data_prep(df_pca) # in case there are more columns added apart from the PCA output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop on kmeans to find the optimum value for k\n",
    "eval_list = []\n",
    "for k in range(2,20):\n",
    "    kmeans_model, predictions, eval_metrics =\\\n",
    "        k_means_spark(df_clust, k=k, getSilhouette=True)\n",
    "    eval_list.append(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_list = pd.DataFrame(data=eval_list, columns=['k','silhouette','sse'])\n",
    "eval_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the elbow plot\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.lineplot(x=eval_list['k'], y=eval_list['sse'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model, predictions, eval_metrics = k_means_spark(df_clust, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the cluster prediction labels added to the data\n",
    "predictions.show(5)\n",
    "predictions.groupBy('prediction').count().sort('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the prediction labels\n",
    "predictions.write.parquet('./interim/predictions/', mode='overwrite')\n",
    "predictions = spark.read.parquet('./interim/predictions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cluster output\n",
    "x_comp = 0\n",
    "y_comp = 1\n",
    "\n",
    "df_plot = df_umap.join(predictions.select('cell','prediction'),on=['cell'],how='left')\n",
    "df_plot = df_plot.select(['cell',str(x_comp),str(y_comp),'prediction']).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(x=df_plot[str(x_comp)], y=df_plot[str(y_comp)], hue=df_plot['prediction'], palette='deep')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP & K-means\n",
    "Perform UMAP on the data, Visualize and perform Kmeans clustering based on the UMAP embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#apply UMAP on HVG dataframe\n",
    "df_umap = umap_apply(top_hvg, n_components=2)\n",
    "\n",
    "# write umap_embeddings to disc\n",
    "df_umap.write.parquet('./interim/df_umap/', mode='overwrite')\n",
    "\n",
    "#read parquet UMAP\n",
    "df_umap = spark.read.parquet('./interim/df_umap/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join the metadata with UMAP embeddings**\n",
    "<br>Use these lines for joining the metadata to any of the previously processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_metadata = spark.read.csv('./data/Jurkat_annotations.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the metadata file\n",
    "%%time\n",
    "cols_rename = [cols.replace('.','_').replace(' ','_').replace('(','_').replace(')','_') for cols in df_metadata.columns]\n",
    "df_metadata = df_metadata.toDF(*cols_rename)\n",
    "\n",
    "# rename the sample/cells column to cells\n",
    "%%time\n",
    "df_metadata = df_metadata.withColumn('cell',F.regexp_replace(F.col('cell'), '[.\\s()]', '_'))\n",
    "\n",
    "# check the count\n",
    "df_metadata.count()\n",
    "\n",
    "# perform the join\n",
    "%%time\n",
    "df_umap = df_umap.join(df_metadata, on=['cell'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualise the UMAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes for components to visualize\n",
    "# starts at 0; x_comp=0 and y_comp=1 plots first component on x-axis and second component on y-axis<br>\n",
    "# update as required\n",
    "x_comp = 0\n",
    "y_comp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract components from umap\n",
    "# skip/update the couluring column in the select function (sp_name)\n",
    "df_plot = df_umap.select(['cell',str(x_comp), str(y_comp), 'sp_nme']).toPandas()\n",
    "\n",
    "# plot UMAP\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(x=df_plot[str(x_comp)], y=df_plot[str(y_comp)], hue = df_plot['sp_nme'])\n",
    "plt.xlabel('Umap 1')\n",
    "plt.ylabel('umap 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KMeans**\n",
    "<br>clustering- takes Vector assembled column of features to perform clustering\n",
    "<br>(use the cluster_prep fn in case the input features aren't Vector assembled already- only PCA is already vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust = cluster_data_prep(df_umap.select('cell','0','1'))\n",
    "df_clust.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop on Kmeans to determine the optimum K\n",
    "eval_list = []\n",
    "for k in range(2,20):\n",
    "    kmeans_model, predictions, eval_metrics =\\\n",
    "        k_means_spark(df_clust, k=k, getSilhouette=True)\n",
    "    eval_list.append(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_list = pd.DataFrame(data=eval_list, columns=['k','silhouette','sse'])\n",
    "eval_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the elbow curve\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.lineplot(x=eval_list['k'], y=eval_list['sse'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model, predictions, eval_metrics = k_means_spark(df_clust, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkint the cluster prediction labels added to the data\n",
    "predictions.show(5)\n",
    "predictions.groupBy('prediction').count().sort('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the prediction labels\n",
    "predictions.write.parquet('./interim/predictions/', mode='overwrite')\n",
    "predictions = spark.read.parquet('./interim/predictions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cluster output\n",
    "x_comp = 0\n",
    "y_comp = 1\n",
    "\n",
    "df_plot = df_umap.join(predictions.select('cell','prediction'),on=['cell'],how='left')\n",
    "df_plot = df_plot.select(['cell',str(x_comp),str(y_comp),'prediction']).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(x=df_plot[str(x_comp)], y=df_plot[str(y_comp)], hue=df_plot['prediction'], palette='deep')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE & KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#apply tsne\n",
    "df_tsne = tsne_apply(top_hvg, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write quantile normalized tsne embeddings\n",
    "df_tsne.write.parquet('./interim/df_tsne/', mode='overwrite')\n",
    "\n",
    "# read tsne from quantile folder\n",
    "df_tsne = spark.read.parquet('./interim/df_tsne/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join the metadata with t-SNE embeddings**\n",
    "<br>Use these lines for joining the metadata to any of the previously processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#jurkat_metadata\n",
    "df_metadata = spark.read.csv('D:/data/Jurkat_annotations.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the metadata file\n",
    "%%time\n",
    "cols_rename = [cols.replace('.','_').replace(' ','_').replace('(','_').replace(')','_') for cols in df_metadata.columns]\n",
    "df_metadata = df_metadata.toDF(*cols_rename)\n",
    "\n",
    "# rename the sample/cells column to cells\n",
    "%%time\n",
    "df_metadata = df_metadata.withColumn('cell',F.regexp_replace(F.col('cell'), '[.\\s()]', '_'))\n",
    "\n",
    "# check the count\n",
    "df_metadata.count()\n",
    "\n",
    "# perform the join\n",
    "%%time\n",
    "df_umap = df_tsne.join(df_metadata, on=['cell'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the tSNE embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starts at 0; x_comp=0 and y_comp=1 plots first component on x-axis and second component on y-axis\n",
    "#update as required\n",
    "x_comp = 0\n",
    "y_comp = 1\n",
    "\n",
    "\n",
    "#extract components from tsne\n",
    "# skip/update the couluring column in the select function (sp_name)\n",
    "df_plot = df_tsne.select(['cell',str(x_comp), str(y_comp), 'sp_nme']).toPandas()\n",
    "\n",
    "# plot tsne\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(x=df_plot[str(x_comp)], y=df_plot[str(y_comp)], hue = df_plot['sp_nme'])\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kmeans**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust = cluster_data_prep(df_tsne.select('cell','0','1'))\n",
    "df_clust.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop on Kmeans to determine the optimum K\n",
    "eval_list = []\n",
    "for k in range(2,20):\n",
    "    kmeans_model, predictions, eval_metrics =\\\n",
    "        k_means_spark(df_clust, k=k, getSilhouette=True)\n",
    "    eval_list.append(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_list = pd.DataFrame(data=eval_list, columns=['k','silhouette','sse'])\n",
    "eval_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the elbow curve\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.lineplot(x=eval_list['k'], y=eval_list['sse'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model, predictions, eval_metrics = k_means_spark(df_clust, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkint the cluster prediction labels added to the data\n",
    "predictions.show(5)\n",
    "predictions.groupBy('prediction').count().sort('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the prediction labels\n",
    "predictions.write.parquet('./interim/predictions/', mode='overwrite')\n",
    "predictions = spark.read.parquet('./interim/predictions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cluster output\n",
    "x_comp = 0\n",
    "y_comp = 1\n",
    "\n",
    "df_plot = df_umap.join(predictions.select('cell','prediction'),on=['cell'],how='left')\n",
    "df_plot = df_plot.select(['cell',str(x_comp),str(y_comp),'prediction']).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(x=df_plot[str(x_comp)], y=df_plot[str(y_comp)], hue=df_plot['prediction'], palette='deep')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differential expression of selected genes across clusters\n",
    "<br>\n",
    "this uses post-QC pre-norm data- df_melt_flt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt_flt = spark.read.parquet('./interim/df_melt_flt/')\n",
    "predictions = spark.read.parquet('./interim/predictions/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add cluster labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt_flt = df_melt_flt.join(predictions.select('cell','prediction'), on=['cell'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call the DE function for selected cluster id and N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_genes = diff_expr_top_n(df_melt_flt,cluster_id=0,n=10)\n",
    "cluster_1_genes = diff_expr_top_n(df_melt_flt,cluster_id=1,n=10)\n",
    "cluster_2_genes = diff_expr_top_n(df_melt_flt,cluster_id=2,n=10)\n",
    "cluster_3_genes = diff_expr_top_n(df_melt_flt,cluster_id=3,n=10)\n",
    "cluster_4_genes = diff_expr_top_n(df_melt_flt,cluster_id=4,n=10)\n",
    "cluster_5_genes = diff_expr_top_n(df_melt_flt,cluster_id=5,n=10)\n",
    "#cluster_6_genes = diff_expr_top_n(df_melt_flt,cluster_id=6,n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the top 10 genes for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_genes\n",
    "cluster_0_genes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3_genes\n",
    "cluster_3_genes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_2_genes\n",
    "cluster_2_genes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_1_genes\n",
    "cluster_1_genes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_5_genes\n",
    "cluster_5_genes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
